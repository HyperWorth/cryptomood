import logging
import time
import pytz
import re
from playwright.sync_api import sync_playwright
from urllib.parse import urljoin  # Base URL eklemek iÃ§in
from datetime import datetime
from dateutil import parser  # Ã‡eÅŸitli tarih formatlarÄ±nÄ± iÅŸlemek iÃ§in
from database import insert_article

# Logging ayarlarÄ±
logging.basicConfig(
    filename='scraper.log', 
    level=logging.ERROR, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding="utf-8"
)

max_scroll_attempts = 20  # Maksimum scroll sayÄ±sÄ±
scroll_delay = 2  # Scroll sonrasÄ± bekleme sÃ¼resi (saniye)
timeout = 5000  # `wait_for_selector` iÃ§in maksimum bekleme sÃ¼resi (ms)

def fetch_filtered_hrefs(url, filters, base_url, unwanted_filters):
    Completed_Url_list = []
    """
    Belirli URL'ler iÃ§in verilen filtrelerle href'leri alÄ±r.
    
    :param url: URL'yi belirtir.
    :param filters: URL'ye Ã¶zel filtreler listesi (her URL iÃ§in farklÄ± filtreler belirlenebilir).
    :param base_url: Href'lerin eksik olduÄŸu durumda eklenecek base URL.
    :param unwanted_filters: Ä°stenmeyen URL'leri iÃ§eren filtreler listesi.
    """
    try:
        with sync_playwright() as p:
            # Chromium tarayÄ±cÄ±sÄ±nÄ± baÅŸlat
            browser = p.chromium.launch(headless=True)  # headless=False yaparak tarayÄ±cÄ±yÄ± gÃ¶rÃ¼nÃ¼r yapabilirsin
            # Yeni bir context (Ã§alÄ±ÅŸma ortamÄ±) oluÅŸturun ve User-Agent'Ä± ayarlayÄ±n
            context = browser.new_context(
            storage_state=None,  # Ã‡erezleri, Ã¶nbelleÄŸi ve oturum verilerini saklamaz
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        )
            
            # Yeni bir sayfa oluÅŸturun
            page = context.new_page()
            # SayfayÄ± yÃ¼kle
            page.goto(url)
            page.wait_for_load_state('load')  # SayfanÄ±n tamamen yÃ¼klenmesini bekleyelim
            time.sleep(5)  # SayfanÄ±n yÃ¼klenmesini bekleyelim
            # ğŸ“Œ 1ï¸âƒ£ Ã–nce SayfayÄ± En AÅŸaÄŸÄ± KaydÄ±r
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(3)  # Ä°Ã§eriÄŸin yÃ¼klenmesini bekleyelim
            # ğŸ“Œ 2ï¸âƒ£ Sonra SayfayÄ± En YukarÄ± KaydÄ±r
            page.evaluate("window.scrollTo(0, 0)")
            time.sleep(2)  # SayfanÄ±n tekrar oturmasÄ±nÄ± bekleyelim
            for _ in range(max_scroll_attempts):
                # Ã–nce <a> etiketlerinin yÃ¼klenip yÃ¼klenmediÄŸini kontrol et
                if page.locator("a").count() > 0:
                    #print("âœ… Linkler yÃ¼klendi, kaydÄ±rma iÅŸlemi durduruluyor.")
                    break  # Linkler yÃ¼klendiyse dÃ¶ngÃ¼yÃ¼ kÄ±r
                    
                #print("âŒ Linkler henÃ¼z yÃ¼klenmedi, kaydÄ±rmaya devam ediliyor...")

                # SayfayÄ± aÅŸaÄŸÄ± kaydÄ±r
                page.evaluate("window.scrollBy(0, window.innerHeight)")
                time.sleep(scroll_delay)


            # Sayfadaki tÃ¼m <a> etiketlerini seÃ§
            links = page.query_selector_all('a')

            # URL'leri saklamak iÃ§in bir set oluÅŸtur
            url_set = set()

            # Her bir linki kontrol et
            for link in links:
                href = link.get_attribute('href')
                if href:
                    # EÄŸer href tam bir URL deÄŸilse (Ã¶rn. /business/ gibi), base URL'yi ekle
                    if not href.startswith('http'):
                        href = urljoin(base_url, href)
                    
                    # Filtreleri kontrol et
                    for filter_pattern in filters:
                        if filter_pattern in href:  # `startswith` yerine `in` kullanarak daha geniÅŸ bir eÅŸleÅŸme saÄŸlÄ±yoruz
                            
                            if href == "https://cryptonews.com/news/":
                                #print(f"Istenmeyen tam eÅŸleÅŸen URL bulundu, atlanÄ±yor: {href}")
                                break
                            # Ä°stenmeyen URL'leri kontrol et
                            if any(unwanted in href for unwanted in unwanted_filters):
                                #print(f"Istenmeyen URL bulundu, atlanÄ±yor: {href}")
                                break  # Bu URL'yi atla, diÄŸer URL'lere geÃ§
                            url_set.add(href)  # Set'e ekle (tekrar etmez)

            # SonuÃ§ olarak kaÃ§ tane URL alÄ±ndÄ±ÄŸÄ±nÄ± yazdÄ±r
            #print(f"Toplam alÄ±nan URL sayÄ±sÄ± ({url}): {len(url_set)}")
            # Debug ekleyebilirsiniz
            # URL'leri yazdÄ±r
            for filtered_url in url_set:
                Completed_Url_list.append(filtered_url)
            context.close()
            browser.close()
            return Completed_Url_list
    except Exception as e:
        logging.error(f"URL Ã§ekme iÅŸlemi sÄ±rasÄ±nda hata oluÅŸtu ({url}): {e}")
        return []

def convert_to_turkey_time(raw_date):
    """
    FarklÄ± formatlarda gelen tarih bilgisini TÃ¼rkiye saatine Ã§evirir.

    :param raw_date: Ã‡ekilen tarih (string)
    :return: TÃ¼rkiye saatine Ã§evrilmiÅŸ tarih (string) veya hata durumunda orijinal hali
    """
    istanbul_tz = pytz.timezone("Europe/Istanbul")

    try:
        # Gereksiz metinleri (Ã¶rneÄŸin "Published") ve Ã¶zel boÅŸluk karakterlerini temizle
        raw_date = re.sub(r"(Published|Updated|on|at|)", "", raw_date).strip()  # "Published", "Updated" gibi metinleri temizle
        raw_date = raw_date.replace("\u202f", " ").strip()  # \u202f gibi Ã¶zel boÅŸluklarÄ± dÃ¼zgÃ¼n boÅŸluÄŸa Ã§evir

        # GMT zaman dilimini kaldÄ±r (Ã¶rneÄŸin "GMT+3")
        raw_date = re.sub(r" GMT[+\-]\d+", "", raw_date).strip()

        # UTC ifadesini kaldÄ±r
        raw_date = raw_date.replace("UTC", "").strip()

        # "p.m." ve "a.m." gibi ifadeleri dÃ¶nÃ¼ÅŸtÃ¼rme (Ã¶nce kÃ¼Ã§Ã¼k harfe, sonra doÄŸru formatta)
        raw_date = raw_date.replace("p.m.", "PM").replace("a.m.", "AM")

        # EÄŸer tarih direkt ISO 8601 formatÄ±nda (Ã¶rneÄŸin: 2025-03-20T15:30:00Z) geliyorsa
        if "T" in raw_date or "Z" in raw_date:
            date_obj = datetime.fromisoformat(raw_date.replace("Z", "+00:00"))

        # EÄŸer tarih formatÄ± "Mar 20, 2025, 6:06 p.m. UTC" ÅŸeklindeyse
        elif "," in raw_date and any(x in raw_date for x in ["AM", "PM"]):
            date_obj = datetime.strptime(raw_date, "%b %d, %Y, %I:%M %p")

        # EÄŸer tarih formatÄ± "20 Mart 2025 18:15" ÅŸeklindeyse
        elif "Mart" in raw_date or "March" in raw_date:
            # TÃ¼rkÃ§e ay isimlerini kullanabilmek iÃ§in locale ayarÄ± yapÄ±lmasÄ± gerekebilir.
            # Bunun iÃ§in datetime'e yerel bir ayar eklemek gerekebilir.
            try:
                date_obj = datetime.strptime(raw_date, "%d %B %Y %H:%M")
            except ValueError:
                # EÄŸer TÃ¼rkÃ§e ay ismi hata verirse, string yerine sayÄ±larla da iÅŸlem yapÄ±labilir.
                raw_date = raw_date.replace("Mart", "March")
                date_obj = datetime.strptime(raw_date, "%d %B %Y %H:%M")

        # Tarih formatÄ±nÄ± otomatik algÄ±lamak iÃ§in dateutil.parser'Ä± kullan
        else:
            date_obj = parser.parse(raw_date)

        # EÄŸer UTC veya baÅŸka bir zaman diliminde ise, TÃ¼rkiye saatine Ã§evir
        if date_obj.tzinfo is None:
            date_obj = pytz.utc.localize(date_obj)  # EÄŸer zaman dilimi yoksa UTC olarak kabul et

        date_obj = date_obj.astimezone(istanbul_tz)  # UTC'den TÃ¼rkiye zaman dilimine Ã§evir

        # SonuÃ§ olarak TÃ¼rkiye saatiyle formatlanmÄ±ÅŸ tarihi dÃ¶ndÃ¼r
        return date_obj.strftime("%Y-%m-%d %H:%M:%S")  # Format: YYYY-MM-DD HH:MM:SS

    except Exception as e:
        logging.error(f"âŒ Tarih formatÄ± dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼rken hata oluÅŸtu: {raw_date} | Hata: {e}")
        return raw_date  # Hata durumunda orijinal formatÄ± dÃ¶ndÃ¼r

def fetch_article_date(page, base_url):
    """
    Sayfadaki makalenin yayÄ±nlanma tarihini Ã§eker ve TÃ¼rkiye saatine Ã§evirir.

    :param page: Playwright sayfa nesnesi.
    :param base_url: Makalenin ait olduÄŸu temel site URL'si.
    :return: Makalenin yayÄ±nlanma tarihi (string, TÃ¼rkiye saati).
    """
    selector = date_selectors.get(base_url, None)

    if selector:
        try:
            date_element = page.locator(selector)
            if date_element.count() > 0:
                raw_date = date_element.inner_text().strip()
                return convert_to_turkey_time(raw_date)

        except Exception as e:
             logging.error(f"Hata: {e}")

    return "Tarih bulunamadÄ±"

def fetch_article_content(article_url, base_url, content_div_class):
    """
    Verilen makale URL'sine giderek belirli bir div iÃ§indeki <p> etiketlerini toplar.
    
    :param article_url: Makale URL'si
    :param content_div_class: Ä°Ã§eriÄŸin bulunduÄŸu div'in class ismi
    :return: Makalenin iÃ§eriÄŸi (string)
    """
    try:
        with sync_playwright() as p:           
            try:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                storage_state=None,  
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            )
                page = context.new_page()

                # Makale sayfasÄ±na git
                page.goto(article_url)
                page.wait_for_load_state('load')
                time.sleep(5)  # SayfanÄ±n yÃ¼klenmesini bekleyelim
                # ğŸ“Œ 1ï¸âƒ£ Ã–nce SayfayÄ± En AÅŸaÄŸÄ± KaydÄ±r
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(3)  # Ä°Ã§eriÄŸin yÃ¼klenmesini bekleyelim
                # ğŸ“Œ 2ï¸âƒ£ Sonra SayfayÄ± En YukarÄ± KaydÄ±r
                page.evaluate("window.scrollTo(0, 0)")
                time.sleep(2)  # SayfanÄ±n tekrar oturmasÄ±nÄ± bekleyelim

                for _ in range(max_scroll_attempts):
                    # Ã–nce iÃ§eriÄŸin yÃ¼klenip yÃ¼klenmediÄŸini kontrol et
                    if page.locator(f"{content_div_class}").count() > 0:
                        #print("âœ… Makale iÃ§eriÄŸi yÃ¼klendi, kaydÄ±rma iÅŸlemi durduruluyor.")
                        break  # Ä°Ã§erik yÃ¼klendiyse dÃ¶ngÃ¼yÃ¼ kÄ±r
                        
                    #print("âŒ Ä°Ã§erik henÃ¼z yÃ¼klenmedi, kaydÄ±rmaya devam ediliyor...")

                    # SayfayÄ± aÅŸaÄŸÄ± kaydÄ±r
                    page.evaluate("window.scrollBy(0, window.innerHeight)")
                    time.sleep(scroll_delay)
                
                article_date = fetch_article_date(page, base_url)

                # **Sadece belirli div iÃ§indeki <p> etiketlerini seÃ§iyoruz**
                paragraphs = page.locator(f"{content_div_class}").all_text_contents()
                
                # ParagraflarÄ± tek bir string haline getir
                article_text = "\n".join(paragraphs)

                #print(f"\nâœ… Makale AlÄ±ndÄ±: {article_url}\n")
                #print(article_text[:500])  # Ä°lk 500 karakteri gÃ¶ster (kontrol iÃ§in)

            except Exception as e:
                logging.error(f"âŒ Hata oluÅŸtu ({article_url}): {e}")
                article_text = None
                article_date = None
                
            context.close()
            browser.close()
            insert_article(article_url, article_date, article_text)
            return article_url ,article_date, article_text
    except Exception as e:
        logging.error(f"Makale iÃ§eriÄŸi alÄ±nÄ±rken hata oluÅŸtu ({article_url}): {e}")
        return article_url, None, None


date_selectors = {
    "https://www.coindesk.com": ".font-metadata.flex.gap-4.text-charcoal-600.flex-col.md\:block > span.md\:ml-2",
    "https://cryptonews.com": ".single-post-new__author-top-value time",
}